input_queries: "data/questions.csv" # file with a list of queries to use for evaluation

# Evaluation results are written to the "results_folder" folder.
# You can override the names of files in this folder by specifying 'generated_answers', 'eval_results_file', and 'metrics_file'.
results_folder: "data/open_rag_eval_results/"
generated_answers: "rag_results.csv" # Point to the pre-existing results (relative to results_folder)
eval_results_file: "eval_results.csv"
metrics_file: "metrics.png"

evaluator:
  - type: "TRECEvaluator"
    model:
      type: "OpenAIModel"
      name: "gpt-4o-mini"
      api_key: ${oc.env:OPENAI_API_KEY} # Reads from environment variable.
    options:
      # The k values to evaluate metrics like precision@k at.
      k_values: [1, 3, 5]
      run_consistency: False # Set to False since we only have 1 run per query
      metrics_to_run_consistency: []
# No connector section - we're using pre-existing results from converted_results.csv
